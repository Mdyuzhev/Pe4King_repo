# Архитектура LLM-адаптера

Документ описывает архитектуру интеграции Pe4King с языковыми моделями для улучшения сгенерированных тестов.

---

## Назначение

Pe4King генерирует скелетные тесты детерминистически — на основе анализа OpenAPI-схемы. Это быстро, предсказуемо и не требует внешних зависимостей. Однако детерминистический подход имеет потолок качества: скелет проверяет структуру ответа, но не понимает бизнес-логику.

LLM-адаптер расширяет возможности генератора, позволяя опционально улучшить скелетные тесты с помощью языковой модели. Ключевое слово — опционально. Базовая функциональность Pe4King работает полностью автономно, LLM выступает как необязательный enhancer.

---

## Принципы проектирования

**Провайдер-агностичность.** Адаптер не привязан к конкретной модели или API. Пользователь выбирает провайдера в настройках — облачный сервис, локальную модель, или корпоративный endpoint. Смена провайдера не требует изменения кода приложения.

**Graceful degradation.** Если LLM недоступен (нет ключа, нет сети, таймаут), система продолжает работать в базовом режиме. Пользователь получает скелетные тесты вместо ошибки.

**Прозрачность.** Пользователь видит что именно отправляется в LLM и что получено в ответ. Никакой магии — полный контроль над промптами и результатами.

**Безопасность.** API-ключи хранятся в защищённом хранилище операционной системы, не в конфигурационных файлах. Чувствительные данные из спецификаций (примеры с реальными данными, внутренние URL) могут быть замаскированы перед отправкой.

---

## Архитектура

### Уровни абстракции

```
┌─────────────────────────────────────────────────────────────────┐
│                        Pe4King Core                              │
│                                                                  │
│   Generator ──→ TestModel ──→ Renderer ──→ Output               │
│       │                                                          │
│       │ (optional)                                               │
│       ▼                                                          │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                    LlmEnhancer                           │   │
│   │                                                          │   │
│   │   enhance(skeleton, schema, config) → enhancedCode      │   │
│   └──────────────────────────┬──────────────────────────────┘   │
│                              │                                   │
└──────────────────────────────┼───────────────────────────────────┘
                               │
                               ▼
┌─────────────────────────────────────────────────────────────────┐
│                       LlmAdapter                                 │
│                                                                  │
│   ┌───────────────┐  ┌───────────────┐  ┌───────────────┐       │
│   │ PromptBuilder │  │ LlmProvider   │  │ ResponseParser│       │
│   │               │  │  (interface)  │  │               │       │
│   │ buildPrompt() │  │  complete()   │  │ extractCode() │       │
│   └───────────────┘  └───────┬───────┘  └───────────────┘       │
│                              │                                   │
└──────────────────────────────┼───────────────────────────────────┘
                               │
                               ▼
┌─────────────────────────────────────────────────────────────────┐
│                   Конкретные провайдеры                          │
│                                                                  │
│   ┌─────────────┐  ┌─────────────┐  ┌─────────────┐             │
│   │  Provider A │  │  Provider B │  │  Provider C │  ...        │
│   │  (cloud)    │  │  (local)    │  │  (custom)   │             │
│   └─────────────┘  └─────────────┘  └─────────────┘             │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Интерфейс провайдера

Каждый провайдер реализует единый интерфейс, скрывая детали конкретного API.

```
LlmProvider
├── complete(prompt, options) → response
├── isAvailable() → boolean
├── getModelInfo() → ModelInfo
└── estimateTokens(text) → number
```

Метод `complete` принимает текстовый промпт и опции (максимальное количество токенов, температура, stop-последовательности), возвращает текстовый ответ модели. Метод `isAvailable` проверяет доступность провайдера — есть ли ключ, отвечает ли endpoint. Метод `getModelInfo` возвращает информацию о модели для отображения в UI. Метод `estimateTokens` позволяет оценить размер промпта до отправки.

### Конфигурация провайдера

```
LlmProviderConfig
├── type: string              // идентификатор провайдера
├── endpoint: string          // URL API (для кастомных endpoint'ов)
├── model: string             // название модели
├── apiKeyRef: string         // ссылка на ключ в SecretStorage
├── timeout: number           // таймаут запроса в секундах
├── maxTokens: number         // лимит токенов ответа
└── customHeaders: Map        // дополнительные заголовки
```

Конфигурация хранится в настройках плагина/расширения. API-ключ хранится отдельно в защищённом хранилище и извлекается по ссылке `apiKeyRef` в момент запроса.

---

## Компоненты адаптера

### PromptBuilder

Формирует промпт для конкретной задачи. Разные задачи требуют разных промптов.

```
PromptBuilder
├── buildEnhancePrompt(skeleton, schema, language) → string
├── buildNegativeTestPrompt(endpoint, schema) → string
├── buildEdgeCasePrompt(endpoint, schema) → string
└── buildExplainPrompt(testCode) → string
```

Промпт включает системную инструкцию (роль модели, ограничения), контекст (схема endpoint'а, существующий код), и конкретную задачу (что нужно сделать). Формат промпта унифицирован — разные провайдеры могут требовать разную структуру (system/user/assistant), но PromptBuilder абстрагирует эти различия.

### ResponseParser

Извлекает структурированные данные из ответа модели.

```
ResponseParser
├── extractCode(response, language) → string
├── extractExplanation(response) → string
├── extractAssertions(response) → List<Assertion>
└── validate(code, language) → ValidationResult
```

Модель может вернуть код в markdown-блоках, с пояснениями до и после, с несколькими вариантами. ResponseParser извлекает именно код, убирает markdown-разметку, валидирует синтаксис. Если ответ невалиден — возвращает ошибку с описанием проблемы.

### LlmEnhancer

Высокоуровневый компонент, который использует адаптер для конкретных задач улучшения тестов.

```
LlmEnhancer
├── enhanceSkeleton(test, schema) → EnhancedTest
├── generateNegativeTests(endpoint) → List<Test>
├── suggestEdgeCases(endpoint) → List<EdgeCase>
└── explainTest(testCode) → Explanation
```

LlmEnhancer координирует работу: строит промпт через PromptBuilder, отправляет через LlmProvider, парсит ответ через ResponseParser, обрабатывает ошибки и возвращает результат в формате, понятном остальной системе.

---

## Сценарии использования

### Улучшение скелетного теста

Пользователь сгенерировал скелетные тесты и хочет улучшить конкретный тест с помощью LLM.

```
1. Пользователь выбирает тест в UI
2. Нажимает "Улучшить с помощью AI"
3. LlmEnhancer получает:
   ├── Код скелетного теста
   ├── Response schema endpoint'а
   └── Настройки улучшения (глубина, фокус)
4. PromptBuilder формирует промпт
5. LlmProvider отправляет запрос
6. ResponseParser извлекает улучшенный код
7. UI показывает diff: было → стало
8. Пользователь принимает или отклоняет изменения
```

### Генерация негативных тестов

Скелетный генератор создаёт только happy-path тесты. LLM может предложить негативные сценарии.

```
1. Пользователь выбирает endpoint
2. Нажимает "Сгенерировать негативные тесты"
3. LlmEnhancer анализирует:
   ├── Параметры endpoint'а
   ├── Типы и ограничения полей
   └── Возможные коды ошибок из spec
4. LLM генерирует тесты для:
   ├── Невалидных параметров
   ├── Отсутствующих обязательных полей
   ├── Граничных значений
   └── Некорректных типов
5. Пользователь выбирает какие тесты добавить
```

### Объяснение существующего теста

Для анализа чужого кода или обучения новых членов команды.

```
1. Пользователь открывает тест в EVA
2. Нажимает "Объяснить тест"
3. LLM получает код теста
4. Возвращает объяснение:
   ├── Что тест проверяет
   ├── Какие assertions используются
   ├── Какие сценарии покрыты
   └── Что можно улучшить
```

---

## Формат промптов

### Структура промпта

```
┌─────────────────────────────────────────────────────────────────┐
│ SYSTEM                                                          │
│                                                                  │
│ Ты — эксперт по тестированию API. Твоя задача — улучшать       │
│ автоматические тесты, добавляя проверки на основе схемы        │
│ ответа. Генерируй только код, без пояснений. Сохраняй стиль    │
│ и форматирование исходного теста.                               │
└─────────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────────┐
│ USER                                                            │
│                                                                  │
│ ## Исходный тест                                                │
│ ```java                                                         │
│ {skeleton_code}                                                 │
│ ```                                                             │
│                                                                  │
│ ## Response Schema                                              │
│ ```json                                                         │
│ {response_schema}                                               │
│ ```                                                             │
│                                                                  │
│ ## Задача                                                       │
│ Добавь assertions для всех полей из schema. Используй           │
│ подходящие matchers для каждого типа данных.                    │
└─────────────────────────────────────────────────────────────────┘
```

### Шаблоны промптов

Промпты хранятся как шаблоны с плейсхолдерами. Это позволяет пользователю кастомизировать промпты без изменения кода.

```
templates/
├── enhance_test.txt
├── negative_tests.txt
├── edge_cases.txt
└── explain_test.txt
```

Плейсхолдеры: `{skeleton_code}`, `{response_schema}`, `{endpoint_path}`, `{http_method}`, `{language}`, `{framework}`.

---

## Обработка ошибок

### Типы ошибок

```
LlmError
├── ConnectionError      // нет связи с провайдером
├── AuthenticationError  // невалидный ключ
├── RateLimitError       // превышен лимит запросов
├── TimeoutError         // провайдер не ответил вовремя
├── InvalidResponseError // ответ не содержит валидный код
└── QuotaExceededError   // исчерпан баланс/квота
```

### Стратегия обработки

При возникновении ошибки система не падает, а gracefully деградирует. Пользователь получает понятное сообщение и возможность продолжить работу без LLM.

```
try {
    enhanced = llmEnhancer.enhance(skeleton, schema)
    showDiff(skeleton, enhanced)
} catch (e: LlmError) {
    showNotification("LLM недоступен: ${e.message}")
    showOriginalSkeleton(skeleton)  // продолжаем без улучшения
}
```

Для RateLimitError можно реализовать автоматический retry с exponential backoff. Для QuotaExceededError — предложить сменить провайдера или использовать локальную модель.

---

## Безопасность

### Хранение ключей

API-ключи хранятся в защищённом хранилище операционной системы. В IntelliJ это CredentialStore API, который использует системный keychain. В VS Code это SecretStorage API. Ключи никогда не пишутся в лог, не отображаются в UI в открытом виде, не сохраняются в файлах настроек.

### Маскирование данных

OpenAPI-спецификации могут содержать чувствительные данные в примерах: реальные email-адреса, внутренние URL, токены. Перед отправкой в LLM такие данные могут быть замаскированы.

```
Исходный пример в spec:
"email": "john.doe@company-internal.com"

После маскирования:
"email": "user@example.com"
```

Маскирование опционально и настраивается пользователем.

### Аудит запросов

Опционально можно логировать все запросы к LLM (без ключей) для аудита и отладки. Логи хранятся локально и не отправляются никуда.

---

## Интеграция в UI

### IntelliJ Plugin

Интеграция добавляет элементы в существующие панели.

В GenerateTestsDialog появляется чекбокс "Улучшить с помощью AI" и выпадающий список выбора провайдера. В контекстном меню на сгенерированном файле появляется пункт "Enhance with AI". В EvaPanel рядом с рекомендациями появляется кнопка "Автоисправление" для автоматического применения рекомендаций через LLM.

В настройках плагина появляется секция "AI Integration" с полями: выбор провайдера, ввод API-ключа (с кнопкой "Сохранить в Keychain"), настройка endpoint'а для кастомных провайдеров, выбор модели, настройка таймаута и лимитов.

### VS Code Extension

Аналогичная интеграция в webview-панели.

Команда в Command Palette: "Pe4King: Enhance Test with AI". Кнопка в панели генерации. Секция в settings.json (без ключей — только ссылка на SecretStorage).

---

## Расширяемость

### Добавление нового провайдера

Для поддержки нового LLM-провайдера нужно реализовать интерфейс LlmProvider. Основная работа — маппинг между унифицированным форматом запроса и конкретным API провайдера.

```
NewProvider implements LlmProvider {
    complete(prompt, options) {
        // 1. Преобразовать prompt в формат API
        // 2. Добавить заголовки авторизации
        // 3. Отправить HTTP-запрос
        // 4. Преобразовать ответ в унифицированный формат
    }
}
```

Новый провайдер регистрируется в ProviderRegistry и становится доступен в UI без изменения остального кода.

### Добавление новой задачи

Для поддержки новой задачи (например, генерация документации к тестам) нужно добавить шаблон промпта и метод в LlmEnhancer.

```
LlmEnhancer {
    generateDocumentation(testCode) {
        prompt = promptBuilder.buildDocPrompt(testCode)
        response = provider.complete(prompt)
        return responseParser.extractMarkdown(response)
    }
}
```

---

## Метрики и мониторинг

### Собираемые метрики

Для анализа эффективности LLM-интеграции собираются анонимные метрики (опционально, с согласия пользователя).

```
LlmMetrics
├── requestCount          // количество запросов
├── successRate           // процент успешных ответов
├── averageLatency        // средняя задержка
├── tokenUsage            // использование токенов
├── acceptanceRate        // процент принятых улучшений
└── errorsByType          // распределение ошибок
```

Метрики помогают понять: насколько полезны улучшения, какой провайдер работает лучше, где узкие места.

---

## Ограничения

**Качество зависит от модели.** Разные модели дают разное качество улучшений. Маленькие локальные модели могут генерировать некомпилируемый код. Документ не гарантирует качество — только архитектуру интеграции.

**Latency.** LLM-запросы занимают секунды, иногда десятки секунд. Для batch-генерации сотен тестов это может быть неприемлемо. LLM лучше подходит для точечного улучшения отдельных тестов.

**Стоимость.** Облачные провайдеры тарифицируют по токенам. При интенсивном использовании счета могут быть значительными. Локальные модели бесплатны, но требуют ресурсов.

**Детерминизм.** LLM недетерминистичен — один и тот же промпт может давать разные результаты. Для воспроизводимости можно фиксировать seed (если провайдер поддерживает), но полной воспроизводимости не гарантируется.

---

## Автор

Mikhail Dyuzhev

---

*Документ: LLM_ADAPTER_ARCHITECTURE.md*
*Версия: 1.0*
*Проект: Pe4King*
